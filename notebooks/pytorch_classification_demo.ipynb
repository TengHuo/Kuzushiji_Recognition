{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "import gc\n",
    "from albumentations import (Compose, ShiftScaleRotate, RGBShift, Cutout, RandomCrop, PadIfNeeded, Resize)\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kuzushiji.resnet_model import CharacterResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\", context=\"notebook\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 7\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images:3881\n"
     ]
    }
   ],
   "source": [
    "input_path = Path(\"../input\")\n",
    "train_imgs_path = input_path / \"train_images\"\n",
    "print(\"Train Images:%d\" % len(list(train_imgs_path.glob(\"*jpg\"))))\n",
    "train = pd.read_csv(\"../input/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100241706_00004_2</td>\n",
       "      <td>U+306F 1231 3465 133 53 U+304C 275 1652 84 69 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100241706_00005_1</td>\n",
       "      <td>U+306F 1087 2018 103 65 U+304B 1456 1832 40 73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100241706_00005_2</td>\n",
       "      <td>U+306F 572 1376 125 57 U+306E 1551 2080 69 68 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100241706_00006_1</td>\n",
       "      <td>U+3082 1455 3009 65 44 U+516B 1654 1528 141 75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100241706_00007_2</td>\n",
       "      <td>U+309D 1201 2949 27 33 U+309D 1196 1539 27 36 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_id                                             labels\n",
       "0  100241706_00004_2  U+306F 1231 3465 133 53 U+304C 275 1652 84 69 ...\n",
       "1  100241706_00005_1  U+306F 1087 2018 103 65 U+304B 1456 1832 40 73...\n",
       "2  100241706_00005_2  U+306F 572 1376 125 57 U+306E 1551 2080 69 68 ...\n",
       "3  100241706_00006_1  U+3082 1455 3009 65 44 U+516B 1654 1528 141 75...\n",
       "4  100241706_00007_2  U+309D 1201 2949 27 33 U+309D 1196 1539 27 36 ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3881 entries, 0 to 3880\n",
      "Data columns (total 2 columns):\n",
      "image_id    3881 non-null object\n",
      "labels      3605 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 60.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some `labels` seem to contain `NaN` in `train`.  \n",
    "We check showing top-6 images containing `NaN` at `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>100241706_00038_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>100241706_00039_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>100241706_00039_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>100241706_00040_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>100249371_00003_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>100249371_00004_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id labels\n",
       "66  100241706_00038_2    NaN\n",
       "67  100241706_00039_1    NaN\n",
       "68  100241706_00039_2    NaN\n",
       "69  100241706_00040_1    NaN\n",
       "73  100249371_00003_2    NaN\n",
       "74  100249371_00004_1    NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nan_labels = train[train[\"labels\"].isnull()]\n",
    "train_nan_labels.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 276 entries, 66 to 3880\n",
      "Data columns (total 2 columns):\n",
      "image_id    276 non-null object\n",
      "labels      0 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_nan_labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(20, 80))\n",
    "# for i in range(6):\n",
    "#     image_id = train_nan_labels[\"image_id\"].iloc[i]\n",
    "#     file_name = image_id + \".jpg\"\n",
    "#     train_img_path = train_imgs_path / file_name\n",
    "#     train_img = np.asarray(Image.open(train_img_path))\n",
    "#     fig.add_subplot(1, 6, i+1, title=file_name)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.imshow(train_img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seem to contain no characters in `NaN` label's images.  \n",
    "Therefore, we can delete them all and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3605 entries, 0 to 3604\n",
      "Data columns (total 2 columns):\n",
      "image_id    3605 non-null object\n",
      "labels      3605 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 56.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train = train.dropna()\n",
    "train = train.reset_index(drop=True)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **3605** training images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Characters Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100241706_00004_2</td>\n",
       "      <td>U+306F 1231 3465 133 53 U+304C 275 1652 84 69 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100241706_00005_1</td>\n",
       "      <td>U+306F 1087 2018 103 65 U+304B 1456 1832 40 73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100241706_00005_2</td>\n",
       "      <td>U+306F 572 1376 125 57 U+306E 1551 2080 69 68 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100241706_00006_1</td>\n",
       "      <td>U+3082 1455 3009 65 44 U+516B 1654 1528 141 75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100241706_00007_2</td>\n",
       "      <td>U+309D 1201 2949 27 33 U+309D 1196 1539 27 36 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_id                                             labels\n",
       "0  100241706_00004_2  U+306F 1231 3465 133 53 U+304C 275 1652 84 69 ...\n",
       "1  100241706_00005_1  U+306F 1087 2018 103 65 U+304B 1456 1832 40 73...\n",
       "2  100241706_00005_2  U+306F 572 1376 125 57 U+306E 1551 2080 69 68 ...\n",
       "3  100241706_00006_1  U+3082 1455 3009 65 44 U+516B 1654 1528 141 75...\n",
       "4  100241706_00007_2  U+309D 1201 2949 27 33 U+309D 1196 1539 27 36 ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Data Description](https://www.kaggle.com/c/kuzushiji-recognition/data),\n",
    "\n",
    "> The string should be read as space separated series of values where `Unicode character`, `X`, `Y`, `Width`, and `Height` are repeated as many times as necessary.\n",
    "\n",
    "We create a dictionary `train_chars` where the key is `image_id` and the value is a dictionary containing `Unicode character`, `X`, `Y`, `Width` and `Height`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4acba914adb40f7a7e78d99a316b4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3605), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "683464"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chars = {}\n",
    "uc_list = []\n",
    "train_chars_num = 0\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    image_id = train.iloc[i][\"image_id\"]\n",
    "    labels = train.iloc[i][\"labels\"].split(\" \")\n",
    "    values = {\"Unicode\" : [],\n",
    "              \"X\" : [],\n",
    "              \"Y\" : [],\n",
    "              \"Width\" : [],\n",
    "              \"Height\" : []}\n",
    "    for j in range(0, len(labels), 5):\n",
    "        uc = labels[j]\n",
    "        x = int(labels[j+1])\n",
    "        y = int(labels[j+2])\n",
    "        w = int(labels[j+3])\n",
    "        h = int(labels[j+4])\n",
    "        uc_list.append(uc)\n",
    "        values[\"Unicode\"].append(uc)\n",
    "        values[\"X\"].append(x)\n",
    "        values[\"Y\"].append(y)\n",
    "        values[\"Width\"].append(w)\n",
    "        values[\"Height\"].append(h)\n",
    "        train_chars_num += 1\n",
    "    train_chars[image_id] = values\n",
    "train_chars_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100241706_00004_2\n",
      "(1231, 3465, 133, 53)\n"
     ]
    }
   ],
   "source": [
    "the_id = list(train_chars.keys())\n",
    "the_id = the_id[0]\n",
    "the_label = train_chars[the_id]\n",
    "print(the_id)\n",
    "print(\"({}, {}, {}, {})\".format(the_label['X'][0],\n",
    "                                the_label['Y'][0],\n",
    "                                the_label['Width'][0],\n",
    "                                the_label['Height'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_id, labels):\n",
    "    image = Image.open('../input/train_images/{}.jpg'.format(image_id))\n",
    "    xy_list = []\n",
    "    labels_zip = zip(labels[\"Unicode\"],\n",
    "                     labels[\"X\"],\n",
    "                     labels[\"Y\"],\n",
    "                     labels[\"Width\"],\n",
    "                     labels[\"Height\"])\n",
    "    for uc, x, y, w, h in labels_zip:\n",
    "        # Crop as Character's PIL Image\n",
    "        char_img = image.crop((x, y, x+w, y+h))\n",
    "        # Add Training Data\n",
    "        char_img = np.asarray(char_img)\n",
    "        xy_list.append((char_img, uc))\n",
    "    return xy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.99 s, sys: 4.8 s, total: 9.79 s\n",
      "Wall time: 31.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "683464"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pool = multiprocessing.Pool(processes=8)\n",
    "result = []\n",
    "for key, values in train_chars.items():\n",
    "    result.append(pool.apply_async(read_image, (key, values,)))\n",
    "pool.close()\n",
    "pool.join()\n",
    "char_uc_list = []\n",
    "for res in result:\n",
    "    xy_list = res.get()\n",
    "    char_uc_list += xy_list\n",
    "\n",
    "len(char_uc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.93389556728664 77.20407804946566 3.0\n"
     ]
    }
   ],
   "source": [
    "image_size = []\n",
    "for image_uc in char_uc_list:\n",
    "    image_size.append(image_uc[0].shape)\n",
    "image_size = np.array(image_size)\n",
    "\n",
    "print(image_size[:,0].mean(), image_size[:,1].mean(), image_size[:,2].mean())\n",
    "del image_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get **683464** character images(seems to be too large).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Unicodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4787 entries, 0 to 4786\n",
      "Data columns (total 2 columns):\n",
      "Unicode    4787 non-null object\n",
      "char       4787 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 74.9+ KB\n"
     ]
    }
   ],
   "source": [
    "uc_trans = pd.read_csv(input_path / \"unicode_translation.csv\")\n",
    "uc_trans.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4787 classes of all unicode characters.  \n",
    "However, some characters might be useless in training images.  \n",
    "We check useless unicodes which are in `uc_trans[\"Unicode\"]` and are not in all unicodes of `train_chars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4212 entries, 5 to 4786\n",
      "Data columns (total 2 columns):\n",
      "Unicode    4212 non-null object\n",
      "char       4212 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 98.7+ KB\n"
     ]
    }
   ],
   "source": [
    "uc_list = set(uc_list)\n",
    "uc_trans = uc_trans[uc_trans['Unicode'].isin(uc_list)]\n",
    "uc_trans.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be 575 useless unicodes in training images.  \n",
    "Finally, we shrink `uc_trans` from 4787 classes to **4212**(=4787-575)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of unicode `uc_list` whose index is used for training and test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uc_list = uc_trans[\"Unicode\"].values.tolist()\n",
    "uc_list.index(\"U+306F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Pytorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `KuzushijiCharDataset` class extended from `torch.utils.data.Dataset`.  \n",
    "As it costs little time to get i-th training data, it creates as follows.\n",
    "1. Open PIL Image each `image_id`\n",
    "2. Crop as Character's PIL Image\n",
    "3. Resize Character's PIL Image\n",
    "4. Gray-Scale Character's PIL Image where the channel is 1\n",
    "5. Convert from Character's PIL Image to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuzushijiCharDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_list: list,\n",
    "                 uc_list: list,\n",
    "                 image_path: Path,\n",
    "                 scale_resize: tuple,\n",
    "                 transform=None):\n",
    "        self._image_list = []\n",
    "        self._y_list = []\n",
    "        self.scale_resize = scale_resize\n",
    "        self.transform = transform\n",
    "                \n",
    "        for image, uc in tqdm(data_list):\n",
    "            self._image_list.append(image)\n",
    "            # Add Training Label\n",
    "            uc_idx = uc_list.index(uc)\n",
    "            self._y_list.append(uc_idx)\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._y_list)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        char_img = self._image_list[idx]\n",
    "        \n",
    "#         old_img = char_img.copy()\n",
    "#         old_img = Image.fromarray(old_img)\n",
    "#         old_img = old_img.resize(self.scale_resize)\n",
    "#         old_img = transforms.functional.to_tensor(old_img)\n",
    "        \n",
    "#         if self.transform:\n",
    "#             char_img = self.transform(image=char_img)['image']\n",
    "#         else:\n",
    "#             # Resize Character's PIL Image\n",
    "#             char_img = Image.fromarray(char_img)\n",
    "#             char_img = char_img.resize(self.scale_resize)\n",
    "\n",
    "        char_img = self.transform(image=char_img)['image']\n",
    "        # Convert to Tensor\n",
    "        x = transforms.functional.to_tensor(char_img)\n",
    "        y = self._y_list[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some width or height are too large.  \n",
    "For the time being, we decide the resizing scale by fixed values(=48)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615117\n",
      "68347\n"
     ]
    }
   ],
   "source": [
    "xy_train, xy_val = train_test_split(char_uc_list, test_size=0.1)\n",
    "print(len(xy_train))\n",
    "print(len(xy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_resize = 64\n",
    "h_resize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. RGBShift\n",
    "# 2. ShiftScaleRotate\n",
    "# 3. Resize\n",
    "# 3. Cutout\n",
    "\n",
    "def train_augment(p=.5):\n",
    "    return Compose([\n",
    "        RGBShift(r_shift_limit=100, g_shift_limit=100, b_shift_limit=100, p=p),\n",
    "        Cutout(num_holes=10, max_h_size=5, max_w_size=5, fill_value=0, p=p),\n",
    "        PadIfNeeded(min_height=90, min_width=70, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0), always_apply=True),\n",
    "        RandomCrop(height=h_resize, width=w_resize, p=p),\n",
    "        Resize(height=h_resize, width=w_resize, always_apply=True)\n",
    "    ], p=p)\n",
    "\n",
    "def valid_augment():\n",
    "    return Compose([\n",
    "        PadIfNeeded(min_height=90, min_width=70, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0), always_apply=True),\n",
    "        Resize(height=h_resize, width=w_resize, always_apply=True)\n",
    "    ], p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_transform_train = train_augment()\n",
    "char_transform_valid = valid_augment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i_batch, sample_batched in enumerate(dataloaders['train']):\n",
    "#     old_image, image, _ = sample_batched\n",
    "#     image = np.array(image[0])\n",
    "#     image = np.moveaxis(image, 0, 2)\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.title('new')\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "\n",
    "#     old_image = np.array(old_image[0])\n",
    "#     old_image = np.moveaxis(old_image, 0, 2)\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.title(\"old\")\n",
    "#     plt.imshow(old_image)\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Demonstration for Classifying Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = labels.size(0)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloaders, num_epochs, checkpoints_path):\n",
    "    history = {'train': [], 'valid': []}\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, factor=0.15, patience=3)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=0.001)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=3)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "\n",
    "            for phase in ['train', 'valid']:\n",
    "                metrics = {'loss': [], 'acc': []}\n",
    "                torch.cuda.empty_cache()\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                    for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        acc = accuracy(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        metrics['loss'].append(loss.data.cpu().numpy())\n",
    "                        metrics['acc'].append(acc)\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "                    with torch.no_grad():\n",
    "                        for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                            inputs, labels = inputs.to(device), labels.to(device)\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            acc = accuracy(outputs, labels)\n",
    "                            metrics['loss'].append(loss.data.cpu().numpy())\n",
    "                            metrics['acc'].append(acc)\n",
    "                history[phase].append({k: np.mean(metrics[k]) for k in metrics})\n",
    "                print(phase, history[phase][-1])\n",
    "            # scheduler.step(history['valid'][-1]['loss'])\n",
    "            scheduler.step()\n",
    "            torch.save(model.state_dict(), checkpoints_path.format(epoch))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    torch.save(model.state_dict(), checkpoints_path.format('final'))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharacterResnet(\n",
      "  (resnet_layer): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "  (fc_): Linear(in_features=2048, out_features=4212, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "model = CharacterResnet(resnet)\n",
    "model.load_state_dict(torch.load('../checkpoints/resnet50_start4.pth'))\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd09cac8eb204bc9bea0d66b1563dbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=615117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2867a5c9ff264e56b9d5f76dde3761f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=68347), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 30\n",
    "checkpoint_path = '../checkpoints/resnet50_{}.pth'\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "dataloaders = {'train': DataLoader(KuzushijiCharDataset(data_list=xy_train,\n",
    "                                                        uc_list=uc_list,\n",
    "                                                        image_path=train_imgs_path,\n",
    "                                                        scale_resize=(w_resize, h_resize),\n",
    "                                                        transform=char_transform_train),\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   shuffle=True,\n",
    "                                   num_workers=8),\n",
    "               'valid': DataLoader(KuzushijiCharDataset(data_list=xy_val,\n",
    "                                                        uc_list=uc_list,\n",
    "                                                        image_path=train_imgs_path,\n",
    "                                                        scale_resize=(w_resize, h_resize),\n",
    "                                                        transform=char_transform_valid),\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=8)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c8f4c37c144499bc009591a9ee77a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1202), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train {'loss': 0.33499017, 'acc': 0.9329340977055518}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f67753632424cfeada5c1346f6f4fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=134), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid {'loss': 0.124525696, 'acc': 0.9791563689272761}\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef6deaace09449c9f0f562b28aee864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1202), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train {'loss': 0.33141693, 'acc': 0.933876521536768}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12710c9d527e4262b801ade9907ab9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=134), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid {'loss': 0.124759264, 'acc': 0.979243822285485}\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1ea4b48c0c491f86e5a30eaa81d7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1202), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train {'loss': 0.32935408, 'acc': 0.934498880486283}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d182373bd44cd0a523fbe6699f8c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=134), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid {'loss': 0.12481664, 'acc': 0.9790980666884701}\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702c932eb305426a871828c73e5458c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1202), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train {'loss': 0.33027813, 'acc': 0.9341121473103771}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ceeb52ffbf443493f3431cee3cba6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=134), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "valid {'loss': 0.12735432, 'acc': 0.979068915569067}\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee7033c43ac4e43a3ddaa4d35e38203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1202), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 40s, sys: 7min 46s, total: 29min 27s\n",
      "Wall time: 29min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model, history = train(model, dataloaders, max_epochs, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tr = pd.DataFrame({'train_' + k: [metrics[k] for metrics in history['train']] for k in history['train'][0]})\n",
    "history_val = pd.DataFrame({'val_' + k: [metrics[k] for metrics in history['valid']] for k in history['valid'][0]})\n",
    "df_history = pd.concat([history_tr, history_val], 1)\n",
    "df_history.to_csv('../cache/resnet_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa21573ee80>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_history = pd.read_csv('../cache/resnet_history.csv')\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = plt.axes()\n",
    "# part_history = df_history[[\"train_loss\", \"val_loss\"]]\n",
    "df_history.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
